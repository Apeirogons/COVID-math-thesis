\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Epidemic Timeseries - Thesis}
\author{Matthew So}
\date{October 2020}
\usepackage{graphicx}
\usepackage{mathrsfs}
\begin{document}

\maketitle

\tableofcontents


\section{Relevant resources}

\begin{itemize}
    \item \hyperlink{https://docs.google.com/document/d/105QZAGMzl7m6JZiXGLuycXWrZcsydqJ_/edit}{Meeting planner} 
    \item \hyperlink{https://github.com/Apeirogons/COVID-math-thesis}{Overall thesis GitHub repo}
    \item \hyperlink{https://github.com/Apeirogons/COVID-math-thesis/tree/master/shifts}{Shifts folder}
    \item \hyperlink{https://github.com/Apeirogons/COVID-math-thesis/tree/master/epidemic_timeseries}{Folder for phase-plane analysis, mobility PCA, and clustering}    
\end{itemize}



\section{Mobility PCA}
\subsection{Explanation of mobility PCA}
Google Mobility data contains detailed daily mobility data for each country. We wanted to use this data to make a single mobility index that was applicable to all countries.

\subsection{Current approach}

Google Mobility tracks mobility in terms of a percent change from baseline in retail and recreation (RR), grocery and pharmacy (G), transit stations (T), workplaces (W), residential (R), and parks (P).

Mobility data for all listed metrics except P smoothed with a 7-day window (one reason was to remove weekend effects; note that performing no smoothing has similar results). Data was taken for all countries with $\geq$ 200 datapoints and naively accumulated. Then, the PCA algorithm from \href{https://scikit-learn.org/stable/}{scikit-learn} was fitted to this data. 

The explained variance of principal component (PC) 1 is 0.894, and the explained variance of PC 2 is 0.041. The components of PC 1 were (last run): RR 0.57781228  G 0.40459858 T 0.53877504  W 0.41191798 R -0.20610189, and the components of PC 2 are: RR 0.26836183  G 0.66819787 T -0.35537018 W -0.59375593 P -0.05156992.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{figures/United_States.png}
    \caption{A PCA reconstruction (grey lines) of mobility data for United States.}
    \label{fig:my_label}
\end{figure}


\subsection{Concerns}
No major concerns at this moment. 

Just for documentation purposes: In the past, MS had concerns about a seemingly poor reconstruction accuracy in the latter half of the pandemic. After actually plotting the inverse-transformed PCA-transformed data, I no longer believe this is an issue.


\section{Clustering}
\subsection{Explanation of clustering}
Epidemic timeseries (such as incidence, cumulative case, etc.) curves are qualitatively different. For example, the U.S. incidence timeseries is obviously different from the Canadian incidence timeseries. I would like to algorithmically cluster "similar" timeseries, and hopefully this will give some deeper insight about pandemic responses in different countries. 

\subsection{Current approach}
I intend on trying out several different timeseries distance metrics to determine the pairwise distances between each timeseries. I've currently tried the discrete time warping-Euclidian distance, but other metrics have been \href{https://link.springer.com/chapter/10.1007/978-3-030-28665-1_31}{reported}. In terms of the actual clustering, I need to use an algorithm that can cluster points based on pairwise distance. From some of my experimentation, I thought the DBSCAN algorithm gave some good results. This algorithm seeds a new cluster if it has at least $n$ points that are within $\epsilon$ units from it. Then, if another point is within $\epsilon$ units from a cluster point, it is added to a cluster. If two clusters share a point, they are merged into a single cluster. I define the start of the pandemic as the first point in time at which there are $\geq$ 1 cumulative cases in a country, although I'm not sure of how much this matters (due to the use of DTW)

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{figures/cluster 0.png}
    \includegraphics[scale=0.4]{figures/cluster 3.png}
    \caption{Similar incidence timeseries determined by clustering.}
    \label{fig:my_label}
\end{figure}

\subsection{Concerns}
I don't understand DTW yet. 

\section{Phase-planes and other plots}
\subsection{Explanation of plots}
(For myself): Phase-planes plot two variables that are connected in time against each other. They're sort of similar to the vector field plots that pop up if you search up 'phase plane' in that it shows the evolution of two functions of an underlying variable.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.25]{figures/mobility_vs_new_cases.png}
    \caption{A phase plane of transit mobility against new cases for selected countries}
    \label{fig:my_label}
\end{figure}

\pagebreak


\subsection{Current approach}
I scatter-plot the two variables under consideration, then connect each dot with lines and add arrows showing the direction of the motion. I also make heavier dots at the end of each phase plane. The plots should be cleaned up a bit (for example, it may be worthwhile to smooth incidence curves).

\subsection{Concerns}
JD is colorblind, so MS should use a colorblind palette.

\section{Convolution lag estimation}
\subsection{Explanation}
An older idea looking at the feasibility of determining time lag distributions by determining convolution kernels that could possibly be used to transform one timeseries into another. For example, we could find the kernel that transforms the recorded symptomatic timeseries into the recovered timeseries. I know that you said that this is not a very promising research direction, but it was too tempting to not try this after shifts (as we KNOW the time lag distribution from E->I and can directly evaluate this convolution). 

\subsection{Current approach}
At each step, I convolve the true incidence curve with a filter to attempt to get the new symptomatic cases (using scipy.optimize.minimize to minimize the mean squared error between true incidence curve and convolved symptomatic curve, with only valid convolutions used).
I also use two constraints: filter[0] = 0, and I try to penalize non-smoothness by penalizing the size of the first differences of the filter.
I could also force the kernel to correspond to some parametric distribution (gamma, lognormal, etc.) as I did before. I did not evaluate this idea yet, but it is simple to accomplish. 

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_1/estimated_vs_true_kernels_seir.png}
    \caption{Estimated vs true kernel for E to I transition}
    \label{fig:my_label}
\end{figure}

Since this method should be usable on any kind of time lag, I tried to use it on real data with the incidence/recovery curves. To reduce the fluctuations, I smoothed with a filter of 7 days.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_1/Ontario-reconstructed-convolved.png}
    \caption{Reconstructed recovery curves for Ontario}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_1/Ontario-estimated-recovery-kernel.png}
    \caption{Estimated recovery distribution for Ontario}
    \label{fig:my_label}
\end{figure}

\clearpage


\section{Shifts}
\subsection{Explanation of shifts}
An exploration of JD's idea found  \href{http://dushoff.github.io/notebook/shifts.html}{here}. The idea is to investigate using the methods of Cori and Wallinga to estimate $\mathscr{R}(t)$. We will use deconvolution (currently Richardson-Lucy as used in the \href{https://www.medrxiv.org/content/10.1101/2020.06.18.20134858v3}{Gostic paper}) to provide an estimate of the true incidence curve from the number of new symptomatic infections. 

\subsection{Current approach}
I use code from the \href{https://github.com/cobeylab/Rt_estimation}{Gostic paper} to perform Richardson-Lucy deconvolution in R. I use the \href{https://cran.r-project.org/web/packages/EpiEstim/index.html}{EpiEstim} library to generate $\mathscr{R}(t)$ estimates using the Cori and Wallinga-Tenuis methods. 

Like the Gostic paper, I've used a vanilla SEIR model (We need to be able to replicate the results first).

\begin{align}
\frac{dS}{dt} &= -\beta(t) \frac{S}{N}  I \\ 
\frac{dE}{dt} &= \beta(t) \frac{S}{N} I - \gamma E \\
\frac{dI}{dt} &= \gamma E - \mu I \\ 
\frac{dR}{dt} &= \mu I  
\end{align}

Therefore, these are equations representing the \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935673/}{instantaneous reproductive number}:
$\mathscr{R}_{i}(t)$: Instantaneous reproductive number, $\mathscr{R}_{c}(t)$: Case/cohort reproductive number

\begin{align}
    \mathscr{R}_0 = \beta(t)/\mu \\
    \mathscr{R}_{i}(t) = \beta(t)/\mu \cdot S/N
\end{align}

I've left some equations for the determination of the generation interval here from the addition of exponential distributions of $E \xrightarrow{}I$ and $I \xrightarrow{} R$, exploiting the fact that both the means and variances add up. However, this parameterization isn't being used by any of the code at the moment, and if required, it may be better to parameterize it with $\kappa = 1/k$.

$G$: generation interval distribution \\
$k$: shape parameter of Gamma distribution\\
$\theta$: scale parameter of Gamma distribution\\

\begin{align}
    Mean(E \xrightarrow{}I) = 1/\gamma \\ 
    Mean(I \xrightarrow{}R) = 1/\mu \\ 
    Var(E \xrightarrow{}I) = 1/\gamma^2 \\ 
    Var(I \xrightarrow{}R) = 1/\mu^2 \\
    Mean(G) = 1/\gamma + 1/\mu = k\theta\\ 
    Var(G) = 1/\gamma^2 + 1/\mu^2 = k\theta^2 \\ 
    k =  Mean(\text{G})^2/Var(\text{G}) \\
    \theta = Var(\text{G})/Mean(\text{G})
\end{align}

This equation represents the cohort reproductive number, which will be estimated with the Wallinga method.

\begin{equation}
    \mathscr{R}_{c}(t) = \mathscr{R}_{i}(t) * G
\end{equation}

Furthermore, I have also implemented a simulation based on the  \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5871640/}{renewal equation}, with stochasticity.

\begin{equation}
    I_t = Pois(\mathscr{R}_{i}(t) \sum_{s=0}^t I_{t-s} \omega_s)
\end{equation}

\subsection{Progress}
From preliminary testing, the deconvolution method fits the true incidence quite well. However, further testing is required to see if it might work well in practice. It also works reasonably well with a changing $\mathscr{R}(t)$. 

The Cori method to the deconvoluted incidence also seems to work well, even with noise. The Wallinga-Tenuis method also seems to work well, but is horribly slow.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.7]{figures/incidence.png}
    \caption{Deconvolved incidence, true incidence, and symptom onset curves}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.7]{figures/rt_cori_deconvolved.png}
    \caption{Instantaneous R(t) estimated with the method of Cori.}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.7]{figures/rt_wt.png}
    \caption{Cohort R(t) estimated with the method of Wallinga and Tenuis.}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures_update_1/incidence.png}
    \caption{Deconvolution of incidence works reasonably well even with changing R(t).}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures_update_1/rt_cori_deconvolved.png}
    \caption{Cori estimates track R(t) well after the initial 50 days even with changing R(t).}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{figures_update_1/rt_wt.png}
    \caption{WT estimates of R(t) when R(t) is changing.}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_2/incidence.png}
    \caption{Deconvolution of incidence starts to become much more noisy when noise is added to the observed symptomatic curve.}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_2/rt_cori_deconvolved.png}
    \caption{Cori estimates track R(t) well after the initial 50 days even with the noisy deconvolution).}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures_update_2/rt_wt.png}
    \caption{WT estimates of R(t) with noise}
    \label{fig:my_label}
\end{figure}




\end{document}
