\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb}
\usepackage{graphicx}
\newcommand{\nR}{\mathbb{R}}
\overfullrule=0pt
\title{$\nR$(t) and r(t) Estimation}
\author{Matthew So}
\date{February 2021}

\begin{document}

\maketitle



\tableofcontents

\section{Modelling}
The current model is a discrete-time model based on the SEIR framework, which also models weekday trends in observation data. As real-world COVID-19 data has clear "spiky" periodic trends, it is likely that these trends reflect daily changes in observation (as trends in infection would be heavily smoothed by the incubation period). 

This model has three supported running modes, each related to how much noise is present in the model: fully deterministic, simple observation noise, and full process noise. "Simple observation noise" determines the number of daily observations by sampling from some probability distribution, such that the expected value is lower than a parameter $detectionProb$. There also exists an unsupported "generalized observation noise" mode, which implements the observation noise found in the simple mode and adds process noise to observation transitions.

For the purposes of this model, process noise is defined by having to sample from a probability distribution to determine the time of a transition event; for example, sampling from an exponential distribution to determine a time for a $E \rightarrow I$ transition. Observation noise is defined by having to sample from a probability distribution to determine the number of individuals observed on a given day, given that the number of individuals who "could have" been observed on this day has already been previously decided.

\subsection{Explanations for features in the model}

\subsubsection{Conventions}
The sum of two probability distributions $X$ and $Y$ is denoted as $X+Y$, and is computed as the convolution of their probability density functions. The expected value is denoted as $E[X]$. Temporary variables have self-explanatory names. Also, variables in $camelCase$ are equivalent to variables named $like\_this$. 


\subsubsection{State variable explanations}
All state variables are initialized to 0 except $S \gets 10,000,000$. 

\begin{itemize}
    \item $S$: susceptible.
    \item $E$: exposed, but non-infectious
    \item $I$: infectious
    \item $R$: recovered (or dead)
    \item $O$: cumulative infectious individuals who have been observed 
    \item $t$: time since epidemic start.
\end{itemize}

\subsubsection{Parameter/Function explanations}
\begin{itemize}
    \item $\beta(t)$: Same meaning as in typical SEIR models. Defined in the code with respect to $\nR_0$ and $\mu$ (description of $\mu$ to follow). 
    \item $\nR_0$: Implicitly used to define $\beta(t)$. Set to 2.1 with $t < 150$, 0.99 with $t<200$, 0.9 with $t<300$, else 1.2.
    \item $baseObservationDist$: Probability density function of time for an infection to be observed. This PDF is modified based on the day of the week. Currently set to discrete lognormal distribution with mean=$e^{1.7}$ days, log-SD=0.5 
    \item $incubationDist$: Probability density function of time to go from $E \rightarrow I$. Currently set to Currently set to discrete lognormal distribution with mean=$e^{1.63}$ days, log-SD=0.5 
    \item $infectiousDist$: Probability density function of time to go from $I \rightarrow R$. (time spent infectious). Currently set to exponential distribution with mean=10 days. It has occurred to me that I was supposed to reduce this value, but I forgot.
    \item $\kappa$: 1/(dispersion parameter) in alternatively parameterized negative binomial distribution. \cite{NegBinom} $negBinom(mean, 0)$ is implemented as the Poisson. I chose 0 as the value of $\kappa$, implicitly making all instances of $negBinom$ actually $Poisson$.
    \item $negBinom$: Negative binomial distribution parameterized by (mean, $\kappa$). It has occurred to me that due to using this distribution rather than the binomial distribution, odd behavior could occur (for example, having more observations than people who are infected on a particular day).
    \item $dayScalers$: On each weekday, the probability of observation is multiplied by this value. Set to 1.2 for Monday and Tuesday and 1 otherwise. 
    \item $observationProb$: The total probability that an infected individual will be observed. Set to 0.8.
    \item $t_{max}$: Maximal value of $t$ for simulation. Set to 401.
\end{itemize}

\subsubsection{Derived variable explanations}
\begin{itemize}
    \item $incidence$: The number of new individuals infected today that weren't infected yesterday. At t=0, incidence is set to 10.
    \item $expectedIncidence$: The expected number of individuals that would be infected today. This makes no adjustments for past process noise, and effectively asks "what if process noise ceased to exist today?" A timeseries of $expectedIncidence$ is considered to be \textit{optimally smoothed}.
    \item $\mu$: The reciprocal of the $E[incubationDist]$, analogous to the $E \rightarrow I$ controlling parameter in SEIR model.
\end{itemize}

\subsection{Explanation of model logic}
\subsubsection{Pre-simulation setup}

\begin{enumerate}
    \item Precompute an observation distribution for every day in the week. For each weekday, modify a copy of baseObservationDist such that each weekday in the distribution is multiplied by the corresponding item in dayScalers. Then, renormalize this distribution to have a sum of 1.

    \item For each weekday observation distribution, compute the probability $p$ that $weekdayObservationDist \leq (infectiousDist+recoveryDist)$. Then, that weekday's $adjustedObservationProb \gets obervationProb/p$. 
    
    \item Set values for each state variable. Set $N \gets sum(S, E, I, R)$. Additionally, set $t \gets -1$ for setup purposes.

\end{enumerate}

\subsubsection{Simulation}
These steps are executed for each desired value of t between 0 and $t_{max}$.
\begin{enumerate}
    \item Compute $\nR_t \gets \beta_t/\mu \cdot S/N$. This refers to instantaneous $\nR_t$.
    
    \item Compute the weekday, $t \pmod 7$.
    
    \item If t=0, then initialize $incidence$ to some number. Else, if running in full process noise mode, compute $incidence = negBinom(SI\beta(t)/N, \kappa)$ and $expectedIncidence = SI\beta(t)/N$. Else, compute $expectedIncidence = incidence = SI\beta(t)/N$.
    
    \item Transition event times are put into a separate list with each element at each index representing the number of events at (index) days from now. 
        \begin{enumerate}
            \item Process noise mode: For each incident case today, choose an \linebreak $incubationTime (E \rightarrow I)$, $infectiousTime$, and $observationTime (E \rightarrow O)$ by sampling from $incubationDist$, $infectiousDist$, and the appropriate $weekdayObservationDist$ respectively. 
            
            \item Otherwise: Transition events are distributed amongst all future days with probability equal to their respective probability density functions. $E \rightarrow I$ events are proportional to $incubationDist$, $I \rightarrow R$ is proportional to $incubationDist + infectiousDist$, and $E \rightarrow I$ is proportional to the appropriate $weekdayObservationDist$ scaled by $obervationProb$. Note that the previous idea of enforcing \linebreak $observationTime \leq totalRecoveryTime$ is not accomplished here.

        \end{enumerate}

    \item Handle observation events.
        \begin{enumerate}
            \item Process noise mode/Generalized process noise: For each incident case, $totalRecoveryTime$ is $incubationTime + infectiousTime$ days from now. If $observationTime \leq totalRecoveryTime$, then observe the incident case with probability of that weekday's \linebreak $adjustedObservationProb$. 
            \item Fully deterministic mode: The number of observations added is scaled by $observationProb.$
            \item Simple observation noise: The number of observations added at each point in the future is sampled from $negBinom(n_{obs}, \kappa)$.

        \end{enumerate}
    
    \item Execute all other transition events occurring today. For $S \rightarrow E, S=S-1, E=E+1$. For $E \rightarrow I, E=E-1, I=I+1$. For $I \rightarrow R, I = I-1, R = R+1$. For $E \rightarrow O, O = O+1$
    

    
    \item Store all state variables as well as $incidence$ and $expectedIncidence$ from today.
\end{enumerate}

\subsubsection{Post-simulation}
\begin{enumerate}
    \item Compute the generation interval, $infectiousDist + recoveryDist.$ 
    \item Compute case $\nR(t)$ by backward-convolving instantaneous $\nR(t)$ with the generation interval.
    \item Compute $scaledIncidence$ and $scaledExpectedIncidence$ by multiplying $incidence$ and $expectedIncidence$ (respectively) by $observationProb$.
\end{enumerate}


\subsubsection{Previous simulation paradigm}
Before January 2020, this simulation model was an ordinary differential equation SEIR model, assuming that $E \rightarrow I$  resulted in a possible observation. The number of observed symptomatic cases each day was sampled from a beta-binomial distribution with a maximum size of the floor of the daily incident cases. However, this model did not support process noise, nor did it support the weekly observation trends common in COVID-19 data. However, this model was effective for testing purposes.

\clearpage
\subsection{Simulation Images}
\subsubsection{Full process noise}
This run is fairly typical for runs with process noise, with incidence figures on the order of thousands. $\nR(t)$ doesn't tend to vary much between runs, as the pool of susceptibles remains high for all of these simulations.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/process_2_incidence.png}
    \caption{Full process noise incidence.}
\end{figure}

\clearpage
\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figures/process_2_prevalence.png}
\caption{Full process noise prevalence.}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figures/process_2_Rt.png}
\caption{Full process noise $\nR(t)$.}
\end{figure}

However, it is possible for the infection to die out early, by virtue of the first infectors recovering before infecting anyone. For the following run, the number of people initially infected was set to 1.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figures/process_die1_incidence.png}
\caption{Incidence for a run where the infection dies out early.}
\end{figure}

\subsubsection{Simple observation noise}
Runs using simple observation noise differ slightly, but not to the same extent as runs using full process noise. This mode was implemented for two main reasons. Firstly, there was a need for the accurate comparison of different smoothing and deconvolution methods on $\nR(t)$ estimation. Under the process noise mode, the simulation could randomly exhibit increased or decreased incidence than expected, causing the underlying $\nR(t)$ to not be equal to the $\nR(t)$ that could be estimated using the perfectly smoothed data. Secondly, there was a need for a suitable way to assess the quality of $r(t)$ estimation. $r(t)$ is defined by the logarithmic derivative of the true underlying incidence, so even the $expectedIncidence$ timeseries from the process noise simulation mode resulted in a very noisy underlying $r(t)$. Note that the $expectedIncidence$ timeseries from the following plots are perfectly smooth due to not utilizing process noise here. Runs using generalized observation noise appear similar, but this mode is unsupported and will not be discussed further.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/simple_observation_2_incidence.png}
    \caption{Simple observation noise incidence.}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/simple_observation_2_prevalence.png}
    \caption{Simple observation noise prevalence.}
\end{figure}

\subsubsection{Fully deterministic}
The underlying prevalence of the fully deterministic mode is equivalent to that of the simple observation noise mode. In this mode, the true shape of the symptomatic incidence curve unmodified by noise is revealed. It is also possible to obtain very good results for $r(t)$ estimation using this method.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/Fully deterministic_incidence.png}
    \caption{Fully deterministic incidence.}
\end{figure}


\section{Smoothing}

\subsection{7-Day Smoothing}
One of the simplest methods of smoothing, and yet one that proved to be the most effective at smoothing out weekly cycles. My code uses a 7-day rolling mean, and is centered after smoothing.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/7day_smooth_ deterministic .png}
    \caption{Smoothing of deterministic simulation incidence.}
\end{figure}


\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/7day_smooth_ process_1 .png}
    \caption{Smoothing of full process noise simulation incidence.}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/7day_smooth_ simple_observation_1 .png}
    \caption{Smoothing of simple observation noise simulation incidence.}
\end{figure}

\subsection{Wavelet transform low-pass filter}
The discrete wavelet transform converts a timeseries into the wavelet domain, for which there are high-frequency and low-frequency wavelets. Wavelets are periodic functions localized in space, allowing for modelling of periodicity and spatial features that can change over time. I set all wavelets above a certain level (high-frequency wavelets) to zero in order to perform smoothing, although this seems like a crude approach. This works relatively well for certain wavelet parameters, such as using the db4 wavelet and removing all levels above 3. This has the benefit of being highly flexible, allowing for various levels of smoothing and different choices of wavelets. However, it is not as effective as the 7-day smoothing.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/wavelet_smooth deterministic .png}
    \caption{Smoothing of deterministic simulation incidence.}
\end{figure}


\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/wavelet_smooth process_1 .png}
    \caption{Smoothing of full process noise simulation incidence.}
\end{figure}

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/wavelet_smooth simple_observation_1 .png}
    \caption{Smoothing of simple observation noise simulation incidence.}
\end{figure}



\subsection{Fourier transform low-pass filter}
Similarly to the wavelet transform, the Fourier transform converts a timeseries to the frequency domain. One can set all frequencies above a threshold to zero to perform smoothing. However, this suffers the unique issue of not being very accurate at the beginning and end of the timeseries, and has therefore been largely ignored for the rest of the project.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/fft_smooth_ simple_observation_1 .png}
    \caption{FFT Smoothing of simple observation noise simulation incidence.}
\end{figure}

\section {$\nR(t)$ and $r(t)$ Estimation}
\subsection{Cori $\nR(t)$ Estimation}
The standard for estimating instantaneous $\nR(t)$. This method works extremely well for non-noisy/periodic data, and is also robust to noisy/periodic data due to having an intrinsic 7-day smoothing window. 

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/estim_ 1 .png}
    \caption{Cori estimation on data with simple observation noise. This was done using shifting of the incidence curves.}
\end{figure}


\subsection{Wallinga-Teunis $\nR(t)$ Estimation}
The standard for estimating case $\nR(t)$. This method also works very well for what it is intended to do, the estimation of case $\nR(t)$. However, this method reacts to changes in $\nR(t)$ slower than the Cori method, and the original "Shifts" idea does not appear to be better than simply taking a Cori estimate and shifting it instead. As case $\nR(t)$  is backward-convolved from instantaneous $\nR(t)$ by the generation interval distribution, I had an idea to deconvolve the WT estimates in order to obtain "faster" estimates of case $\nR(t)$. However, I haven't pursued this idea much further yet.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/rt_ 1 .png}
    \caption{$r(t)$ estimation on data with simple observation noise. This was done using shifting of the incidence curves.}
\end{figure}


\subsection{Naive $r(t)$ Estimation}
$r(t)$ can be defined as the logarithmic derivative of incidence. Therefore, simply smoothing the data and calculating the logarithmic derivative of incidence may be sufficient for calculating $r(t)$. I decided to apply N-day smoothing to the naive $r(t)$ estimates as well, in order to get a more stable estimate.

\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figures/rt_ 1 .png}
    \caption{$r(t)$ estimation on data with simple observation noise. This was done using shifting of the incidence curves.}
\end{figure}


\section{Shifts vs. Deconvolution}

\subsection{Shifts}
Despite warnings by Gostic et al \cite{Gostic}. about the use of shifts instead of deconvolution in epidemic data, I have found that shifting timeseries is superior in almost every case where there exists noise in the data. Nonetheless, this section will continue to report on the timeseries deconvolution methods I had been working on.

\subsection{Richardson-Lucy Deconvolution}
Total variation-regularized Richardson-Lucy deconvolution is an iterative method following the equation below: \cite{RLLoss}
\begin{equation}
    o_{k+1} = \frac{i}{o_k * h} * (-h) \frac{o_k}{1-\alpha div(\frac{\nabla o_k}{|\nabla o_k|})}
\end{equation}

where $o$ represents the deconvolved image, $i$ represents the observed image, $h$ represents the point-spread function, $div$ is divergence, $\alpha$ is a regularization parameter, $int_z$ refers to integrating over all pixels of the image, and $*$ is the convolution operator.

While this equation looks complex, it can be derived from a simpler principle: the minimization of the following loss function. 
\begin{equation}
    J(o) = \underbrace{\int_z ((h*o)(x) - i(x) log(h*o)(x))dx}_{\mbox{Poisson log-likelihood}} + \underbrace{\alpha \int_z |\nabla o(x)| dx}_{\mbox{Total Variation regularization}}
\end{equation}


In the epidemiological context, $o$ represents the estimated incidence of infection, $i$ represents the symptomatic incidence, $h$ represents the incubation period distribution, $div$ reduces to the 1-dimensional derivative,  $\int_z$ refers to integrating over all times in the timeseries, and $\nabla$ also reduces to the 1-dimensional derivative.

With regularization, the Richardson-Lucy algorithm will converge to some reasonable value. However, in some cases, early stopping is still desirable. The Cobey Lab used a chi-squared statistic comparing the convolution of the deconvolved image to the observed image to perform early stopping. The total variation regularization was a modification of the Cobey Lab's R code.

\subsection{Optimizer-Based Deconvolutions}
Rather than using the Richardson-Lucy algorithm for deconvolution, one can also optimize the same underlying loss function using conventional optimizers. While this problem is too difficult for most of the SciPy optimizers, it is possible to use the Powell (or potentially other non-gradient optimizers) for this task. One upside of this idea is that it is relatively easy to implement deconvolutions based on new statistical models; for example, the negative binomial loglikelihood was used in place of the Poisson log-likelihood in one of my implementations. However, I didn't find that any optimizer-based deconvolutions were better than Richardson-Lucy deconvolution.

\subsection{Wiener Deconvolution}
Wiener deconvolution performs deconvolution in the Fourier domain (deconvolutions are simple divisions in this domain), but adds an additional factor to the denominator. This suffers from similar problems compared to the previously discussed Fourier transform low-pass filters, and was not pursued further.




\begin{thebibliography}{1}
\bibitem{RLLoss}
https://hal.inria.fr/inria-00070726/document
    
\bibitem{Gostic}
https://doi.org/10.1101/2020.06.18.20134858

\bibitem{NegBinom}
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/NegBinomial.html

\end{thebibliography}
\end{document}
